import os
import streamlit as st
import requests
from bs4 import BeautifulSoup
from sqlalchemy import create_engine, text
from datetime import datetime
import pandas as pd  
import aiohttp
import asyncio
from aiohttp import ClientTimeout
from asyncio import Semaphore

#####################################################################################################################################################
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
#####################################################################################################################################################

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(page_title='PARKüå≥RUN', page_icon=':running:')

# –ü—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
image_path = 'logo.jpg'

# –í—Å—Ç–∞–≤–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
st.image(image_path, caption='', width=250)

# –°–∫—Ä—ã—Ç–∏–µ —Ñ—É—Ç–µ—Ä–∞ –∏ –º–µ–Ω—é
hide_streamlit_style = """
            <style>
            MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# –ó–∞–≥–æ–ª–æ–≤–æ–∫
st.title('–î–æ–º–∞—à–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞')

st.subheader('–û–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∏ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü')

st.markdown('''
–¢—É—Ç –≤–æ–∑–º–æ–∂–Ω–æ –±—É–¥–µ—Ç –∫–∞–∫–æ–µ-—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.  
–°–ø–∏—Å–æ–∫ —Å—Ç—Ä–∞–Ω–∏—Ü:
- –û—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞
- –¢–∞–±–ª–∏—Ü–∞ —Ä–µ–∫–æ—Ä–¥–æ–≤
- –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π –±–µ–≥—É–Ω–æ–≤
- –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å —Ç–∞–±–ª–∏—Ü–µ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤
''')

#####################################################################################################################################################
# –ü–∞—Ä—Å–∏–Ω–≥
#####################################################################################################################################################
main_url = 'https://5verst.ru/results/latest/'
target_runs = ['–ü–µ—Ç–µ—Ä–≥–æ—Ñ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∏–π']

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Ç–∞–π–º–∞—É—Ç –∏ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
MAX_CONCURRENT_REQUESTS = 10  # –ú–∞–∫—Å–∏–º—É–º 10 –∑–∞–ø—Ä–æ—Å–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
TIMEOUT_SECONDS = 30  # –¢–∞–π–º–∞—É—Ç –Ω–∞ –∫–∞–∂–¥—ã–π –∑–∞–ø—Ä–æ—Å - 30 —Å–µ–∫—É–Ω–¥

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —Ç–∞–π–º–∞—É—Ç–æ–º
async def fetch(session, url, semaphore):
    async with semaphore:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        try:
            async with session.get(url, timeout=ClientTimeout(total=TIMEOUT_SECONDS)) as response:
                return await response.text()
        except asyncio.TimeoutError:
            print(f"Timeout for {url}")
            return None

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
async def parse_main_table(session, url, semaphore):
    response_text = await fetch(session, url, semaphore)
    if response_text is None:
        return []

    soup = BeautifulSoup(response_text, 'html.parser')
    table = soup.find('table')
    starts_latest = []

    for row in table.find_all('tr')[1:]:
        cells = row.find_all('td')
        number_cell = cells[0]
        run = number_cell.text.strip().split(' #')[0]
        link = number_cell.find('a')['href'] if number_cell.find('a') else None
        date = cells[1].text.strip()
        starts_latest.append([run, date, link])

    return starts_latest

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∑–∞–±–µ–≥–æ–º
async def parse_run_page(session, run_link, location_name, semaphore):
    response_text = await fetch(session, run_link, semaphore)
    if response_text is None:
        return []

    soup = BeautifulSoup(response_text, 'html.parser')
    run_table = soup.find('table')
    run_data = []

    for run_row in run_table.find_all('tr')[1:]:
        run_cells = run_row.find_all('td')
        number = run_cells[0].get_text(strip=True)
        date_cell = run_cells[1].get_text(strip=True)
        link = run_cells[1].find('a')['href'] if run_cells[1].find('a') else None
        finishers = int(run_cells[2].get_text(strip=True))
        volunteers = int(run_cells[3].get_text(strip=True))
        avg_time = run_cells[4].get_text(strip=True)
        best_female_time = run_cells[5].get_text(strip=True)
        best_male_time = run_cells[6].get_text(strip=True)

        run_data.append([location_name, number, date_cell, link, finishers, volunteers, avg_time, best_female_time, best_male_time])

    return run_data

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤
async def parse_participant_and_volunteer_tables(session, run_protocol_link, run_info, semaphore):
    response_text = await fetch(session, run_protocol_link, semaphore)
    if response_text is None:
        return [], []

    soup = BeautifulSoup(response_text, 'html.parser')
    all_tables = soup.find_all('table')

    # –ó–∞–±–µ–≥: location_name, number, date_cell, link, finishers, volunteers, avg_time, best_female_time, best_male_time
    location_name, number, date_cell, link, finishers, volunteer_count, avg_time, best_female_time, best_male_time = run_info

    participants_data = []
    volunteers_data = []

    # –ü–∞—Ä—Å–∏–º —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤
    participant_table = all_tables[0]
    for row in participant_table.find_all('tr')[1:]:
        cells = row.find_all('td')
        if len(cells) >= 4:
            position = cells[0].get_text(strip=True)
            name_tag = cells[1].find('a')
            name = name_tag.get_text(strip=True) if name_tag else '‚Äî'
            profile_link = name_tag['href'] if name_tag else '‚Äî'
            participant_id = profile_link.split('/')[-1] if profile_link != '‚Äî' else '‚Äî'
            stats_div = cells[1].find('div', class_='user-stat')
            finishes = '‚Äî'
            volunteers = '‚Äî'
            if stats_div:
                stats_spans = stats_div.find_all('span')
                finishes = stats_spans[0].get_text(strip=True).split(' ')[0] if len(stats_spans) > 0 else '‚Äî'
                volunteers = stats_spans[1].get_text(strip=True).split(' ')[0] if len(stats_spans) > 1 else '‚Äî'
            club_tags = cells[1].find_all('span', class_='club-icon')
            clubs = ', '.join([club['title'] for club in club_tags]) if club_tags else '‚Äî'
            age_group = cells[2].get_text(strip=True).split(' ')[0] if cells[2] else '‚Äî'
            age_grade_tag = cells[2].find('div', class_='age_grade')
            age_grade = age_grade_tag.get_text(strip=True) if age_grade_tag else '‚Äî'
            time = cells[3].get_text(strip=True) if cells[3] else '‚Äî'
            achievements = []
            achievements_div = cells[3].find('div', class_='table-achievments')
            if achievements_div:
                achievement_icons = achievements_div.find_all('span', class_='results_icon')
                for icon in achievement_icons:
                    achievements.append(icon['title'])
            participants_data.append([location_name, number, date_cell, link, finishers, volunteer_count, avg_time, best_female_time, best_male_time,
                                      position, name, profile_link, participant_id, clubs, finishes, volunteers, age_group, age_grade, time, ', '.join(achievements)])
            
    # –ü–∞—Ä—Å–∏–º –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤
    volunteer_table = all_tables[1]
    for row in volunteer_table.find_all('tr')[1:]:
        columns = row.find_all('td')
        if len(columns) > 1:
            name_tag = columns[0].find('a')
            name = name_tag.get_text(strip=True) if name_tag else '‚Äî'
            profile_link = name_tag['href'] if name_tag else '‚Äî'
            participant_id = profile_link.split('/')[-1] if profile_link != '‚Äî' else '‚Äî'
            stats_div = columns[0].find('div', class_='user-stat')
            finishes = '‚Äî'
            volunteers = '‚Äî'
            if stats_div:
                stats_spans = stats_div.find_all('span')
                finishes = stats_spans[0].get_text(strip=True).split(' ')[0] if len(stats_spans) > 0 else '‚Äî'
                volunteers = stats_spans[1].get_text(strip=True).split(' ')[0] if len(stats_spans) > 1 else '‚Äî'
            club_tags = columns[0].find_all('span', class_='club-icon')
            clubs = ', '.join([club['title'] for club in club_tags]) if club_tags else '‚Äî'
            volunteer_role_info = columns[1].find('div', class_='volunteer__role')
            if volunteer_role_info:
                first_volunteer_tag = volunteer_role_info.find('span', class_='results_icon')
                first_volunteer_info = first_volunteer_tag['title'] if first_volunteer_tag else '‚Äî'
                role_tag = volunteer_role_info.find_all('span')
                volunteer_role = role_tag[-1].get_text(strip=True) if role_tag else '‚Äî'
            else:
                first_volunteer_info = '‚Äî'
                volunteer_role = '‚Äî'
            volunteers_data.append([location_name, number, date_cell, link, finishers, volunteer_count, avg_time, best_female_time, best_male_time,
                                    name, profile_link, participant_id, finishes, volunteers, clubs, volunteer_role, first_volunteer_info])
    
    return participants_data, volunteers_data

# –û—Å–Ω–æ–≤–Ω–∞—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∑–∞–±–µ–≥–æ–≤
async def get_full_run_data(main_url, target_runs):
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)  # –°–æ–∑–¥–∞—ë–º —Å–µ–º–∞—Ñ–æ—Ä

    async with aiohttp.ClientSession() as session:
        # –ü–∞—Ä—Å–∏–º –æ—Å–Ω–æ–≤–Ω—É—é —Ç–∞–±–ª–∏—Ü—É
        starts_latest = await parse_main_table(session, main_url, semaphore)
        filtered_runs = [run for run in starts_latest if any(location in run[0] for location in target_runs)]

        all_participant_data = []
        all_volunteer_data = []

        # –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—ã–π –∑–∞–±–µ–≥
        for run in filtered_runs:
            run_link = run[2]
            location_name = run[0]
            run_data = await parse_run_page(session, run_link, location_name, semaphore)

            # –°–æ–∑–¥–∞—ë–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤
            tasks = [
                parse_participant_and_volunteer_tables(session, run_info[3], run_info, semaphore)
                for run_info in run_data if run_info[3]
            ]
            results = await asyncio.gather(*tasks)

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for participants, volunteers in results:
                all_participant_data.extend(participants)
                all_volunteer_data.extend(volunteers)

        return all_participant_data, all_volunteer_data
    

# –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —É—á–∞—Å—Ç–Ω–∏–∫–∞
async def parse_participant_page(participant_link, session, semaphore):
    html = await fetch(session, participant_link, semaphore)
    if html is None:
        return []  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
    soup = BeautifulSoup(html, 'html.parser')
    stats_data = []

    # –ù–∞–π–¥—ë–º div —Å –Ω—É–∂–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
    stats_div = soup.find('div', class_='grid grid-cols-2 gap-px bg-black/[0.05]')
    if stats_div:
        stats_items = stats_div.find_all('div', class_='bg-white p-4')

        finishes = stats_items[0].find('span', class_='text-3xl font-semibold tracking-tight').text.strip() if len(stats_items) > 0 else 'N/A'
        volunteers = stats_items[1].find('span', class_='text-3xl font-semibold tracking-tight').text.strip() if len(stats_items) > 1 else 'N/A'
        best_time = stats_items[2].find('span', class_='text-3xl font-semibold tracking-tight').text.strip() if len(stats_items) > 2 else 'N/A'
        best_time_link = stats_items[2].find('a', class_='user-info-park-link')['href'] if len(stats_items) > 2 and stats_items[2].find('a', class_='user-info-park-link') else 'N/A'

        clubs = stats_items[3].find_all('span', class_='club-icon') if len(stats_items) > 3 else []
        clubs_titles = ', '.join([club['title'] for club in clubs])
    else:
        finishes = volunteers = best_time = best_time_link = 'N/A'
        clubs_titles = ''

    # –ù–∞–π–¥—ë–º —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∑–∞–±–µ–≥–æ–≤ –≤ –ü–µ—Ç–µ—Ä–≥–æ—Ñ–µ
    tables = soup.find_all('table')
    if tables:
        # –ü–æ–¥—Å—á—ë—Ç —Ñ–∏–Ω–∏—à–µ–π –≤ –ü–µ—Ç–µ—Ä–≥–æ—Ñ–µ
        peterhof_finishes_count = sum(1 for row in tables[0].find_all('tr')[1:] if '–ü–µ—Ç–µ—Ä–≥–æ—Ñ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∏–π' in row.find_all('td')[1].text.strip()) if len(tables) > 0 else 0

        # –î–ª—è –≤–æ–ª–æ–Ω—Ç—ë—Ä—Å—Ç–≤ —Å–æ–∑–¥–∞—ë–º –º–Ω–æ–∂–µ—Å—Ç–≤–æ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç
        peterhof_volunteer_dates = set()
        if len(tables) > 1:
            for row in tables[1].find_all('tr')[1:]:
                location = row.find_all('td')[1].text.strip()
                if '–ü–µ—Ç–µ—Ä–≥–æ—Ñ –ê–ª–µ–∫—Å–∞–Ω–¥—Ä–∏–π—Å–∫–∏–π' in location:
                    date = row.find_all('td')[0].text.strip()  # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞—Ç—É
                    peterhof_volunteer_dates.add(date)  # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞—Ç—É –≤ –º–Ω–æ–∂–µ—Å—Ç–≤–æ (—É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–∞—Ç—ã)
        peterhof_volunteers_count = len(peterhof_volunteer_dates)  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∞—Ç –≤–æ–ª–æ–Ω—Ç—ë—Ä—Å—Ç–≤
    else:
        peterhof_finishes_count = peterhof_volunteers_count = 0

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    stats_data.append([best_time, finishes, peterhof_finishes_count, volunteers, peterhof_volunteers_count, clubs_titles, best_time_link])
    return stats_data

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
async def get_all_stats_data(df_runners):
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö
    all_stats_data = []

    # –ü—Ä–æ–π–¥–µ–º—Å—è –ø–æ –∫–∞–∂–¥–æ–º—É —É–Ω–∏–∫–∞–ª—å–Ω–æ–º—É —É—á–∞—Å—Ç–Ω–∏–∫—É
    unique_links = [link for link in df_runners['profile_link'].unique() if '5verst.ru/userstats' in link]

    semaphore = Semaphore(MAX_CONCURRENT_REQUESTS)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    async with aiohttp.ClientSession() as session:
        tasks = []
        for link in unique_links:
            tasks.append(parse_participant_page(link, session, semaphore))

        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        results = await asyncio.gather(*tasks)

        # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        for link, parsed_data in zip(unique_links, results):
            if parsed_data:  # –ï—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã
                runner_row = df_runners[df_runners['profile_link'] == link].iloc[0]
                name = runner_row['name']
                profile_link = runner_row['profile_link']
                participant_id = runner_row['participant_id']

                # –î–æ–±–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –æ–±—â–∏–π —Å–ø–∏—Å–æ–∫, –≤–∫–ª—é—á–∞—è –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã df_runners
                for data in parsed_data:
                    all_stats_data.append([name, profile_link, participant_id] + data)

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π —Å–ø–∏—Å–æ–∫ –¥–∞–Ω–Ω—ã—Ö
    return all_stats_data

#####################################################################################################################################################
# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –ë–∞–∑—É –î–∞–Ω–Ω—ã—Ö
#####################################################################################################################################################

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
def save_to_database(df_runners, df_orgs, df_stats, db_url='sqlite:///mydatabase.db'):
    # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
    engine = create_engine(db_url)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–≥—É–Ω–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É 'runners'
    df_runners.to_sql('runners', con=engine, if_exists='replace', index=False)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É 'organizers'
    df_orgs.to_sql('organizers', con=engine, if_exists='replace', index=False)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –±–µ–≥—É–Ω–æ–≤ –≤ —Ç–∞–±–ª–∏—Ü—É 'runners'
    df_stats.to_sql('users', con=engine, if_exists='replace', index=False)

# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–¥
async def update_data():
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –≤—Å–µ—Ö —É—á–∞—Å—Ç–Ω–∏–∫–∞—Ö –∏ –≤–æ–ª–æ–Ω—Ç—ë—Ä–∞—Ö
    all_participant_data, all_volunteer_data = await get_full_run_data(main_url, target_runs)

    # –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è –±–µ–≥—É–Ω–æ–≤
    df_runners = pd.DataFrame(all_participant_data, columns=[
        'run', 'run_number', 'run_date', 'run_link', 'finisher', 'volunteer', 'avg_time',
        'best_female_time', 'best_male_time', 'position', 'name', 'profile_link',
        'participant_id', 'clubs', 'finishes', 'volunteers', 'age_group', 'age_grade',
        'time', 'achievements'
    ])
    df_runners['run_date'] = pd.to_datetime(df_runners['run_date'], dayfirst=True)

    # –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è –≤–æ–ª–æ–Ω—Ç—ë—Ä–æ–≤
    df_orgs = pd.DataFrame(all_volunteer_data, columns=[
        'run', 'run_number', 'run_date', 'run_link', 'finisher', 'volunteer', 'avg_time',
        'best_female_time', 'best_male_time', 'name', 'profile_link', 'participant_id',
        'finishes', 'volunteers', 'clubs', 'volunteer_role', 'first_volunteer_info'
    ])
    df_orgs = (
        df_orgs.groupby([col for col in df_orgs.columns if col != 'volunteer_role'])['volunteer_role']
        .apply(lambda x: ', '.join(x))
        .reset_index()
    )
    df_orgs['run_date'] = pd.to_datetime(df_orgs['run_date'], dayfirst=True)

    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
    all_stats_data = await get_all_stats_data(df_runners)

    # –°–æ–∑–¥–∞—ë–º DataFrame –¥–ª—è –∏—Ç–æ–≥–æ–≤–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    df_stats = pd.DataFrame(all_stats_data, columns=[
        'name', 'profile_link', 'participant_id', 'best_time', 'finishes', 
        'peterhof_finishes_count', 'volunteers', 'peterhof_volunteers_count', 
        'clubs_titles', 'best_time_link'
    ])

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
    save_to_database(df_runners, df_orgs, df_stats)

#####################################################################################################################################################
# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
#####################################################################################################################################################

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –∏–∑ —Å–∞–π—Ç–∞
def get_last_date_from_site():
    url = 'https://5verst.ru/petergofaleksandriysky/results/all/'
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    last_date = soup.find_all('table')[0].find_all('tr')[1].find_all('td')[1].text.strip()

    # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ last_date –∏–∑ —Ñ–æ—Ä–º–∞—Ç–∞ DD.MM.YYYY –≤ –æ–±—ä–µ–∫—Ç datetime
    last_date_site = datetime.strptime(last_date, '%d.%m.%Y').date()
    return last_date_site

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –¥–∞—Ç—ã –∏–∑ –ë–î
def get_last_date_from_db(db_url='sqlite:///mydatabase.db'):
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Ñ–∞–π–ª –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    db_path = db_url.replace('sqlite:///', '')  # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    if not os.path.exists(db_path):
        return None  # –ï—Å–ª–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –Ω–µ—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
    
    
    # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    engine = create_engine(db_url)
    with engine.connect() as connection:
        query = text("SELECT MAX(run_date) FROM runners")  # –ó–∞–º–µ–Ω–∏—Ç—å run_date –Ω–∞ —Ä–µ–∞–ª—å–Ω–æ–µ –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–æ–π
        result = connection.execute(query)
        last_date_db = result.scalar()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ last_date_db –Ω–µ None, —Ç–æ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ –¥–∞—Ç—É
        if last_date_db:
            last_date_db = datetime.strptime(last_date_db, '%Y-%m-%d %H:%M:%S.%f').date()
        else:
            last_date_db = None
    return last_date_db


last_date_site = get_last_date_from_site()
last_date_db = get_last_date_from_db()
    
st.subheader('–ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö')  
# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å–ª–∏ last_date_db –Ω–µ –ø—É—Å—Ç–∞—è
if last_date_db is None:
    st.write('–î–∞–Ω–Ω—ã—Ö –≤ –±–∞–∑–µ –Ω–µ—Ç!')
    if st.button('–û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'):
        st.write('–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...')
        asyncio.run(update_data())
        st.success('–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!')
else:
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –¥–∞—Ç
    if last_date_db != last_date_site:
        st.write(f'–î–∞–Ω–Ω—ã–µ —É—Å—Ç–∞—Ä–µ–ª–∏. –î–∞—Ç–∞ –≤ –±–∞–∑–µ: {last_date_db}, –¥–∞—Ç–∞ –Ω–∞ —Å–∞–π—Ç–µ: {last_date_site}.')
        
        if st.button('–û–±–Ω–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ'):
            st.write('–ù–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö...')
            asyncio.run(update_data())
            st.success('–î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö!')
    else:
        st.markdown(f'''–î–∞–Ω–Ω—ã–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã üëç  
                    –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö: {last_date_db}  
                    –ü–æ—Å–ª–µ–¥–Ω—è—è –¥–∞—Ç–∞ –Ω–∞ —Å–∞–π—Ç–µ: {last_date_site}
                    ''')

#####################################################################################################################################################
# –ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏
#####################################################################################################################################################

# st.subheader('–ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏')        
# # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
# def search_database(query):
#     db_url = 'sqlite:///mydatabase.db'
#     engine = create_engine(db_url)
#     with engine.connect() as connection:
#         # –ò—Å–ø–æ–ª—å–∑—É–µ–º SQL –¥–ª—è –ø–æ–∏—Å–∫–∞
#         query_like = f"%{query}%"
#         sql_query = text("SELECT * FROM runners WHERE name LIKE :query")
#         result = connection.execute(sql_query, {"query": query_like}).fetchall()  # –ü–µ—Ä–µ–¥–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å
#     return result

# # –ü–æ–ª–µ –¥–ª—è –ø–æ–∏—Å–∫–∞
# search_query = st.text_input("–ü–æ–∏—Å–∫ –ø–æ –∏–º–µ–Ω–∏ –±–µ–≥—É–Ω–∞:")

# # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
# if search_query:
#     search_results = search_database(search_query)

#     if search_results:
#         # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame
#         df_results = pd.DataFrame(search_results)  # –ë–µ–∑ —É–∫–∞–∑–∞–Ω–∏—è –∏–º–µ–Ω –∫–æ–ª–æ–Ω–æ–∫
#         st.dataframe(df_results)  # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –≤–∏–¥–µ —Ç–∞–±–ª–∏—Ü—ã
#     else:
#         st.write("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.")